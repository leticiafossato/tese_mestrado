{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T03:27:23.849489Z",
     "start_time": "2022-10-13T03:27:23.830126Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import collections\n",
    "import nltk\n",
    "import urllib\n",
    "import bs4 as bs\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import numpy\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T02:55:07.909952Z",
     "start_time": "2022-10-13T02:55:07.887343Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_files = os.listdir('../top1000_complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T02:55:08.193331Z",
     "start_time": "2022-10-13T02:55:07.948798Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_files2= []\n",
    "for i in list_of_files:\n",
    "    nome = '../top1000_complete/' + i + '/' + 'Documents_xml/' +os.listdir(f'../top1000_complete/{i}/Documents_xml')[0]\n",
    "    list_of_files2.append(nome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T02:55:09.086512Z",
     "start_time": "2022-10-13T02:55:08.248367Z"
    }
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "for file in list_of_files2:\n",
    "    f= open(file,'r')\n",
    "    lines.append(f.readlines())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T02:55:10.125334Z",
     "start_time": "2022-10-13T02:55:10.114603Z"
    }
   },
   "outputs": [],
   "source": [
    "paper1= lines[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T02:55:11.506117Z",
     "start_time": "2022-10-13T02:55:11.454656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAPER>\\n',\n",
       " '  <S sid=\"0\">Advances In Domain Independent Linear Text Segmentation</S>\\n',\n",
       " '  <ABSTRACT>\\n',\n",
       " '    <S sid=\"1\" ssid=\"1\">This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998).</S>\\n',\n",
       " '    <S sid=\"2\" ssid=\"2\">Inter-sentence similarity is replaced by rank in the local context.</S>\\n',\n",
       " '    <S sid=\"3\" ssid=\"3\">Boundary locations are discovered by divisive clustering.</S>\\n',\n",
       " '  </ABSTRACT>\\n',\n",
       " '  <SECTION title=\"1 Introduction\" number=\"1\">\\n',\n",
       " '    <S sid=\"4\" ssid=\"1\">Even moderately long documents typically address several topics or different aspects of the same topic.</S>\\n',\n",
       " '    <S sid=\"5\" ssid=\"2\">The aim of linear text segmentation is to discover the topic boundaries.</S>\\n',\n",
       " '    <S sid=\"6\" ssid=\"3\">The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999), summarization (Reynar, 1998), text understanding, anaphora resolution (Kozima, 1993), language modelling (Morris and Hirst, 1991; Beeferman et al., 1997b) and improving document navigation for the visually disabled (Choi, 2000).</S>\\n',\n",
       " '    <S sid=\"7\" ssid=\"4\">This paper focuses on domain independent methods for segmenting written text.</S>\\n',\n",
       " '    <S sid=\"8\" ssid=\"5\">We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994).</S>\\n',\n",
       " '    <S sid=\"9\" ssid=\"6\">The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similarity matrix.</S>\\n',\n",
       " '    <S sid=\"10\" ssid=\"7\">We propose that the similarity values of short text segments is statistically insignificant.</S>\\n',\n",
       " '    <S sid=\"11\" ssid=\"8\">Thus, one can only rely on their order, or rank, for clustering.</S>\\n',\n",
       " '  </SECTION>\\n',\n",
       " '  <SECTION title=\"2 Background\" number=\"2\">\\n',\n",
       " '    <S sid=\"12\" ssid=\"1\">Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997).</S>\\n',\n",
       " '    <S sid=\"13\" ssid=\"2\">The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976).</S>\\n',\n",
       " '    <S sid=\"14\" ssid=\"3\">They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment.</S>\\n',\n",
       " '    <S sid=\"15\" ssid=\"4\">Implementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999), entity repetition (Kan et al., 1998), semantic similarity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion.</S>\\n',\n",
       " '    <S sid=\"16\" ssid=\"5\">Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994).</S>\\n',\n",
       " '    <S sid=\"17\" ssid=\"6\">Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998).</S>\\n',\n",
       " '    <S sid=\"18\" ssid=\"7\">Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998).</S>\\n',\n",
       " '    <S sid=\"19\" ssid=\"8\">Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998).</S>\\n',\n",
       " '    <S sid=\"20\" ssid=\"9\">The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy.</S>\\n',\n",
       " '  </SECTION>\\n',\n",
       " '  <SECTION title=\"3 Algorithm\" number=\"3\">\\n',\n",
       " '    <S sid=\"21\" ssid=\"1\">Our segmentation algorithm takes a list of tokenized sentences as input.</S>\\n',\n",
       " '    <S sid=\"22\" ssid=\"2\">A tokenizer (Grefenstette and Tapanainen, 1994) and a sentence boundary disambiguation algorithm (Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997) or EAGLE (Reynar et al., 1997) may be used to convert a plain text document into the acceptable input format.</S>\\n',\n",
       " '    <S sid=\"23\" ssid=\"3\">Punctuation and uninformative words are removed from each sentence using a simple regular expression pattern matcher and a stopword list.</S>\\n',\n",
       " '    <S sid=\"24\" ssid=\"4\">A stemming algorithm (Porter, 1980) is then applied to the remaining tokens to obtain the word stems.</S>\\n',\n",
       " '    <S sid=\"25\" ssid=\"5\">A dictionary of word stem frequencies is constructed for each sentence.</S>\\n',\n",
       " '    <S sid=\"26\" ssid=\"6\">This is represented as a vector of frequency counts.</S>\\n',\n",
       " '    <S sid=\"27\" ssid=\"7\">Let fi,i denote the frequency of word j in sentence i.</S>\\n',\n",
       " '    <S sid=\"28\" ssid=\"8\">The similarity between a pair of sentences :1:, y For short text segments, the absolute value of sim(x, y) is unreliable.</S>\\n',\n",
       " '    <S sid=\"29\" ssid=\"9\">An additional occurrence of a common word (reflected in the numerator) causes a disproportionate increase in sim(x, y) unless the denominator (related to segment length) is large.</S>\\n',\n",
       " '    <S sid=\"30\" ssid=\"10\">Thus, in the context of text segmentation where a segment has typically &lt; 100 informative tokens, one can only use the metric to estimate the order of similarity between sentences, e.g. a is more similar to b than c. Furthermore, language usage varies throughout a document.</S>\\n',\n",
       " '    <S sid=\"31\" ssid=\"11\">For instance, the introduction section of a document is less cohesive than a section which is about a particular topic.</S>\\n',\n",
       " '    <S sid=\"32\" ssid=\"12\">Consequently, it is inappropriate to directly compare the similarity values from different regions of the similarity matrix.</S>\\n',\n",
       " '    <S sid=\"33\" ssid=\"13\">In non-parametric statistical analysis, one compares the rank of data sets when the qualitative behaviour is similar but the absolute quantities are unreliable.</S>\\n',\n",
       " '    <S sid=\"34\" ssid=\"14\">We present a ranking scheme which is an adaptation of that described in (O\\'Neil and Denos, 1992).</S>\\n',\n",
       " '    <S sid=\"35\" ssid=\"15\">\\'The contrast of the image has been adjusted to highlight the image features.</S>\\n',\n",
       " '    <S sid=\"36\" ssid=\"16\">Each value in the similarity matrix is replaced by its rank in the local region.</S>\\n',\n",
       " '    <S sid=\"37\" ssid=\"17\">The rank is the number of neighbouring elements with a lower similarity value.</S>\\n',\n",
       " '    <S sid=\"38\" ssid=\"18\">Figure 2 shows an example of image ranking using a 3 x 3 rank mask with output range {0, 8).</S>\\n',\n",
       " '    <S sid=\"39\" ssid=\"19\">For segmentation, we used a 11 x 11 rank mask.</S>\\n',\n",
       " '    <S sid=\"40\" ssid=\"20\">The output is expressed as a ratio r (equation 2) to circumvent normalisation problems (consider the cases when the rank mask is not contained in the image).</S>\\n',\n",
       " '    <S sid=\"41\" ssid=\"21\"># of elements with a lower value To demonstrate the effect of image ranking, the process was applied to the matrix shown in figure 1 to produce figure 32.</S>\\n',\n",
       " '    <S sid=\"42\" ssid=\"22\">Notice the contrast has been improved significantly.</S>\\n',\n",
       " '    <S sid=\"43\" ssid=\"23\">Figure 4 illustrates the more subtle effects of our ranking scheme. r(x) is the rank (1 x 11 mask) of (x) which is a sine wave with decaying mean, amplitude and frequency (equation 3).</S>\\n',\n",
       " '    <S sid=\"44\" ssid=\"24\">The final process determines the location of the topic boundaries.</S>\\n',\n",
       " '    <S sid=\"45\" ssid=\"25\">The method is based on Reynar\\'s maximisation algorithm (Reynar, 1998; Helfman, 1996; Church, 1993; Church and Helfman, 1993).</S>\\n',\n",
       " '    <S sid=\"46\" ssid=\"26\">A text segment is defined by two sentences i, j (inclusive).</S>\\n',\n",
       " '    <S sid=\"47\" ssid=\"27\">This is represented as a square region along the diagonal of the rank matrix.</S>\\n',\n",
       " '    <S sid=\"48\" ssid=\"28\">Let si,j denote the sum of the rank values in a segment and ai,j = (j &#8212;i +1)2 be the inside area.</S>\\n',\n",
       " '    <S sid=\"49\" ssid=\"29\">B = {b1, ...,197-4 is a list of in coherent text segments. sk and ak refers to the sum of rank and area of segment k in B.</S>\\n',\n",
       " '    <S sid=\"50\" ssid=\"30\">D is the inside density of B (see equation 4). ak To initialise the process, the entire document is placed in B as one coherent text segment.</S>\\n',\n",
       " '    <S sid=\"51\" ssid=\"31\">Each step of the process splits one of the segments in B.</S>\\n',\n",
       " '    <S sid=\"52\" ssid=\"32\">The split point is a potential boundary which maximises D. Figure 5 shows a working example.</S>\\n',\n",
       " '    <S sid=\"53\" ssid=\"33\">The number of segments to generate, in, is determined automatically.</S>\\n',\n",
       " '    <S sid=\"54\" ssid=\"34\">Den) is the inside density of n segments and SD(n) , Den) Den-1) is the gradient.</S>\\n',\n",
       " '    <S sid=\"55\" ssid=\"35\">For a document with b potential boundaries, b steps of divisive clustering generates {D(1), ...,D(b+1)} and {bD(2), oD(b+1)} (see figure 6 and 7).</S>\\n',\n",
       " '    <S sid=\"56\" ssid=\"36\">An unusually large reduction in 6D suggests the optiinal clustering has been obtained3 (see n = 10 in the threshold, p+c x to dD (c= 1.2 works well in practice) The running time of each step is dominated by the computation of sk.</S>\\n',\n",
       " '    <S sid=\"57\" ssid=\"37\">Given si,j is constant, our algorithm pre-computes all the values to improve speed performance.</S>\\n',\n",
       " '    <S sid=\"58\" ssid=\"38\">The procedure computes the values along diagonals, starting from the main diagonal and works towards the corner.</S>\\n',\n",
       " '    <S sid=\"59\" ssid=\"39\">The method has a complexity of order 171-5.n2.</S>\\n',\n",
       " '    <S sid=\"60\" ssid=\"40\">Let ri,j refer to the rank value in the rank matrix R and S to the sum of rank matrix.</S>\\n',\n",
       " '    <S sid=\"61\" ssid=\"41\">Given R of size n X 77,, S is computed in three steps (see equation 5).</S>\\n',\n",
       " '    <S sid=\"62\" ssid=\"42\">Figure 8 shows the result of applying this procedure to the rank matrix in figure 5.</S>\\n',\n",
       " '  </SECTION>\\n',\n",
       " '  <SECTION title=\"4 Evaluation\" number=\"4\">\\n',\n",
       " '    <S sid=\"63\" ssid=\"1\">The definition of a topic segment ranges from complete stories (Allan et al., 1998) to summaries (Ponte and Croft, 1997).</S>\\n',\n",
       " '    <S sid=\"64\" ssid=\"2\">Given the quality of an algorithm is task dependent, the following experiments focus on the relative performance.</S>\\n',\n",
       " '    <S sid=\"65\" ssid=\"3\">Our evaluation strategy is a variant of that described in (Reynar, 1998, 71-73) and the TDT segmentation task (Allan et al., 1998).</S>\\n',\n",
       " '    <S sid=\"66\" ssid=\"4\">We assume a good algorithm is one that finds the most prominent topic boundaries.</S>\\n',\n",
       " '    <S sid=\"67\" ssid=\"5\">An artificial test corpus of 700 samples is used to assess the accuracy and speed performance of segmentation algorithms.</S>\\n',\n",
       " '    <S sid=\"68\" ssid=\"6\">A sample is a concatenation of ten text segments.</S>\\n',\n",
       " '    <S sid=\"69\" ssid=\"7\">A segment is the first n sentences of a randomly selected document from the Brown corpus\\'.</S>\\n',\n",
       " '    <S sid=\"70\" ssid=\"8\">A sample is characterised by the range of n. The corpus was generated by an automatic procedure5.</S>\\n',\n",
       " '    <S sid=\"71\" ssid=\"9\">Table 1 presents the corpus statistics. p(erroriref, hyp, k) = p(misslref, hyp, diff, k)p(diffl ref, k)+ (6) p(fairef, hyp, same, k)p(samelref, k) Speed performance is measured by the average number of CPU seconds required to process a test sample6.</S>\\n',\n",
       " '    <S sid=\"72\" ssid=\"10\">Segmentation accuracy is measured by the error metric (equation 6, fa false alarms) proposed in (Beeferman et al., 1999).</S>\\n',\n",
       " '    <S sid=\"73\" ssid=\"11\">Low error probability indicates high accuracy.</S>\\n',\n",
       " '    <S sid=\"74\" ssid=\"12\">Other performance measures include the popular precision and recall metric (PR) (Hearst, 1994), fuzzy PR (Reynar, 1998) and edit distance (Ponte and Croft, 1997).</S>\\n',\n",
       " '    <S sid=\"75\" ssid=\"13\">The problems associated with these metrics are discussed in (Beeferman et al., 1999).</S>\\n',\n",
       " '    <S sid=\"76\" ssid=\"14\">Five degenerate algorithms define the baseline for the experiments.</S>\\n',\n",
       " '    <S sid=\"77\" ssid=\"15\">B&#8222; does not propose any boundaries.</S>\\n',\n",
       " '    <S sid=\"78\" ssid=\"16\">B&#8222; reports all potential boundaries as real boundaries.</S>\\n',\n",
       " '    <S sid=\"79\" ssid=\"17\">B&#8222; partitions the sample into regular segments.</S>\\n',\n",
       " '    <S sid=\"80\" ssid=\"18\">B(i.,?) randomly selects any number of boundaries as real boundaries.</S>\\n',\n",
       " '    <S sid=\"81\" ssid=\"19\">B(r,b) randomly selects b boundaries as real boundaries.</S>\\n',\n",
       " '    <S sid=\"82\" ssid=\"20\">The accuracy of the last two algorithms are computed analytically.</S>\\n',\n",
       " '    <S sid=\"83\" ssid=\"21\">We consider the status of in potential boundaries as a bit string (1 -4 topic boundary).</S>\\n',\n",
       " '    <S sid=\"84\" ssid=\"22\">The terms p(iniss) awl p(fa) in equation 6 corresponds to p(samelk) and p(difflk) = 1 -p(samelk).</S>\\n',\n",
       " '    <S sid=\"85\" ssid=\"23\">Equation 7, 8 and 9 gives the general form of p(samelk), B(r,?) and Berm, respectively\\'.</S>\\n',\n",
       " '    <S sid=\"86\" ssid=\"24\">Table 2 presents the experimental results.</S>\\n',\n",
       " '    <S sid=\"87\" ssid=\"25\">The values in row two and three, four and five are not actually the same.</S>\\n',\n",
       " '    <S sid=\"88\" ssid=\"26\">However, their differences are insignificant according to the Kolmogorov-Smirnov, or KS-test (Press et al., 1992).</S>\\n',\n",
       " '    <S sid=\"89\" ssid=\"27\">We compare three versions of the TextTiling algorithm (Hearst, 1994).</S>\\n',\n",
       " '    <S sid=\"90\" ssid=\"28\">H94(c,d) is Hearst\\'s C implementation with default parameters.</S>\\n',\n",
       " '    <S sid=\"91\" ssid=\"29\">H94(e.7.) uses the recommended parameters k = 6, w = 20.</S>\\n',\n",
       " '    <S sid=\"92\" ssid=\"30\">H94(3,,,) is my implementation of the algorithm.</S>\\n',\n",
       " '    <S sid=\"93\" ssid=\"31\">Experimental result (table 3) shows H94(,,d) and H94(,,) are more accurate than H94(j,,,).</S>\\n',\n",
       " '    <S sid=\"94\" ssid=\"32\">We suspect this is due to the use of a different stopword list and stemming algorithm.</S>\\n',\n",
       " '    <S sid=\"95\" ssid=\"33\">Five versions of Reynar\\'s optimisation algorithm (Reynar, 1998) were evaluated.</S>\\n',\n",
       " '    <S sid=\"96\" ssid=\"34\">R98 and R98(7-&#8222;rn) are exact implementations of his maximisation and minimisation algorithm.</S>\\n',\n",
       " '    <S sid=\"97\" ssid=\"35\">R98(8,,08) is my version of the maximisation algorithm which uses the cosine coefficient instead of dot density for measuring similarity.</S>\\n',\n",
       " '    <S sid=\"98\" ssid=\"36\">It incorporates the optimisations described in section 3.4.</S>\\n',\n",
       " '    <S sid=\"99\" ssid=\"37\">R98(,&#8222;,d0t) is the modularised version of R98 for experimenting with different similarity measures.</S>\\n',\n",
       " '    <S sid=\"100\" ssid=\"38\">R98(,,,,&#8222;) uses a variant of Kozima\\'s semantic similarity measure (Kozima, 1993) to compute block similarity.</S>\\n',\n",
       " '    <S sid=\"101\" ssid=\"39\">Word similarity is a function of word cooccurrence statistics in the given document.</S>\\n',\n",
       " '    <S sid=\"102\" ssid=\"40\">Words that belong to the same sentence are considered to be related.</S>\\n',\n",
       " '    <S sid=\"103\" ssid=\"41\">Given the co-occurrence frequencies f (wi, wi), the transition probability matrix t is computed by equation 10.</S>\\n',\n",
       " '    <S sid=\"104\" ssid=\"42\">Equation 11 defines our spread activation scheme. s denotes the word similarity matrix, x is the number of activation steps and norm(y) converts a matrix y into a transition matrix. x = 5 was used in the experiment.</S>\\n',\n",
       " '    <S sid=\"105\" ssid=\"43\">Experimental result (table 4) shows the cosine coefficient and our spread activation method improved segmentation accuracy.</S>\\n',\n",
       " '    <S sid=\"106\" ssid=\"44\">The speed optimisations significantly reduced the execution time.</S>\\n',\n",
       " '    <S sid=\"107\" ssid=\"45\">We compare three versions of Segmenter (Kan et at, 1998).</S>\\n',\n",
       " '    <S sid=\"108\" ssid=\"46\">K98(p) is the original Perl implementation of the algorithm (version 1.6).</S>\\n',\n",
       " '    <S sid=\"109\" ssid=\"47\">K98(i) is my implementation of the algorithm.</S>\\n',\n",
       " '    <S sid=\"110\" ssid=\"48\">K98(j,,i) is a version of K98(i) which uses a document specific chain breaking strategy.</S>\\n',\n",
       " '    <S sid=\"111\" ssid=\"49\">The distribution of link distances are used to identify unusually long links.</S>\\n',\n",
       " '    <S sid=\"112\" ssid=\"50\">The threshold is a function p + c x VT, of the mean p and variance We found c = 1 works well in practice.</S>\\n',\n",
       " '    <S sid=\"113\" ssid=\"51\">Table 5 summarises the experimental results.</S>\\n',\n",
       " '    <S sid=\"114\" ssid=\"52\">K98 performed performed significantly better than K98(J,).</S>\\n',\n",
       " '    <S sid=\"115\" ssid=\"53\">This is due to the use of a different part-of-speech tagger and shallow parser.</S>\\n',\n",
       " '    <S sid=\"116\" ssid=\"54\">The difference in speed is largely due to the programming languages and term clustering strategies.</S>\\n',\n",
       " '    <S sid=\"117\" ssid=\"55\">Our chain breaking strategy improved accuracy (compare K98(i) with K98(j,a))&#8226; Two versions of our algorithm were developed, C99 and C99(b).</S>\\n',\n",
       " '    <S sid=\"118\" ssid=\"56\">The former is an exact implementation of the algorithm described in this paper.</S>\\n',\n",
       " '    <S sid=\"119\" ssid=\"57\">The latter is given the expected number of topic segments for fair comparison with R98.</S>\\n',\n",
       " '    <S sid=\"120\" ssid=\"58\">Both algorithms used a 11 x 11 ranking mask.</S>\\n',\n",
       " '    <S sid=\"121\" ssid=\"59\">The first experiment focuses on the impact of our automatic termination strategy on C99(b) (table 6).</S>\\n',\n",
       " '    <S sid=\"122\" ssid=\"60\">C99(b) is marginally more accurate than C99.</S>\\n',\n",
       " '    <S sid=\"123\" ssid=\"61\">This indicates our automatic termination strategy is effective but not optimal.</S>\\n',\n",
       " '    <S sid=\"124\" ssid=\"62\">The minor reduction in speed performance is acceptable.</S>\\n',\n",
       " '    <S sid=\"125\" ssid=\"63\">The second experiment investigates the effect of different ranking mask size on the performance of C99 (table 7).</S>\\n',\n",
       " '    <S sid=\"126\" ssid=\"64\">Execution time increases with mask size.</S>\\n',\n",
       " '    <S sid=\"127\" ssid=\"65\">A 1 x 1 ranking mask reduces all the elements in the rank matrix to zero.</S>\\n',\n",
       " '    <S sid=\"128\" ssid=\"66\">Interestingly, the increase in ranking mask size beyond 3 x 3 has insignificant effect on segmentation accuracy.</S>\\n',\n",
       " '    <S sid=\"129\" ssid=\"67\">This suggests the use of extrema for clustering has a greater impact on accuracy than linearising the similarity scores (figure 4).</S>\\n',\n",
       " '    <S sid=\"130\" ssid=\"68\">Experimental result (table 8) shows our algorithm C99 is more accurate than existing algorithms.</S>\\n',\n",
       " '    <S sid=\"131\" ssid=\"69\">A two-fold increase in accuracy and seven-fold increase in speed was achieved (compare C99(b) with R98).</S>\\n',\n",
       " '    <S sid=\"132\" ssid=\"70\">If one disregards segmentation accuracy, H94 has the best algorithmic performance (linear).</S>\\n',\n",
       " '    <S sid=\"133\" ssid=\"71\">C99, K98 and R98 are all polynomial time algorithms.</S>\\n',\n",
       " '    <S sid=\"134\" ssid=\"72\">The significance of our results has been confirmed by both t-test and KS-test.</S>\\n',\n",
       " '  </SECTION>\\n',\n",
       " '  <SECTION title=\"5 Conclusions and future work\" number=\"5\">\\n',\n",
       " '    <S sid=\"135\" ssid=\"1\">A segmentation algorithm has two key elements, a, clustering strategy and a similarity measure.</S>\\n',\n",
       " '    <S sid=\"136\" ssid=\"2\">Our results show divisive clustering (R98) is more precise than sliding window (H94) and lexical chains (K98) for locating topic boundaries.</S>\\n',\n",
       " '    <S sid=\"137\" ssid=\"3\">Four similarity measures were examined.</S>\\n',\n",
       " '    <S sid=\"138\" ssid=\"4\">The cosine coefficient (R98(s,&#8222;0) and dot density measure (R98(m,doo ) yield similar results.</S>\\n',\n",
       " '    <S sid=\"139\" ssid=\"5\">Our spread activation based semantic measure (R98(&#8222;,sa)) improved accuracy.</S>\\n',\n",
       " '    <S sid=\"140\" ssid=\"6\">This confirms that although Kozima\\'s approach (Kozima, 1993) is computationally expensive, it does produce more precise segmentation.</S>\\n',\n",
       " '    <S sid=\"141\" ssid=\"7\">The most significant improvement was due to our ranking scheme which linearises the cosine coefficient,.</S>\\n',\n",
       " '    <S sid=\"142\" ssid=\"8\">Our experiments demonstrate that given insufficient data, the qualitative behaviour of the cosine measure is indeed more reliable than the actual values.</S>\\n',\n",
       " '    <S sid=\"143\" ssid=\"9\">Although our evaluation scheme is sufficient for this comparative study, further research requires a large scale, task independent benchmark.</S>\\n',\n",
       " '    <S sid=\"144\" ssid=\"10\">It would be interesting to compare C99 with the multi-source method described in (Beeferman et al., 1999) using the TDT corpus.</S>\\n',\n",
       " '    <S sid=\"145\" ssid=\"11\">We would also like to develop a linear time and multi-source version of the algorithm.</S>\\n',\n",
       " '  </SECTION>\\n',\n",
       " '  <SECTION title=\"Acknowledgements\" number=\"6\">\\n',\n",
       " '    <S sid=\"146\" ssid=\"1\">This paper has benefitted from the comments of Mary McGee Wood and the anonymous reviewers.</S>\\n',\n",
       " '    <S sid=\"147\" ssid=\"2\">Thanks are due to my parents and department for making this work possible; Jeffrey Reynar for discussions and guidance on the segmentation problem; Hideki Kozima for help on the spread activation measure; Min-Yen Kan and Marti Hearst for their segmentation algorithms; Daniel Oram for references to image processing techniques; Magnus Rattray and Stephen Marsland for help on statistics and mathematics.</S>\\n',\n",
       " '  </SECTION>\\n',\n",
       " '</PAPER>\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-10T00:51:16.847646Z",
     "start_time": "2022-10-10T00:51:16.813741Z"
    }
   },
   "source": [
    "lista3=[]\n",
    "lines2=''\n",
    "for i in range(len(paper1)):\n",
    "    for j in paper1[i]:\n",
    "        lines2= lines2+j\n",
    "    lista3.append(lines2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T02:55:12.933618Z",
     "start_time": "2022-10-13T02:55:12.916794Z"
    }
   },
   "outputs": [],
   "source": [
    "texto=''\n",
    "for i in paper1:\n",
    "    texto = texto + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T03:45:21.319380Z",
     "start_time": "2022-10-13T03:45:21.292969Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessa_intro(corpus: str):\n",
    "    regIter = re.finditer('(<SECTION.*Introduction.*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = re.sub(',','', txt)\n",
    "        txt = re.sub(';','', txt)\n",
    "        txt = re.sub(':','', txt)\n",
    "        txt = re.sub('[ ]+',' ', txt)\n",
    "        txt = re.sub(r'\\([^)]*\\)', \"\",txt)\n",
    "        txt = re.sub(r'\\<(.*?)\\>',\"\",txt)\n",
    "        txt = re.sub(r'small\\&',\"\",txt)\n",
    "        txt = re.sub(r'tagger&amp',\"\",txt)\n",
    "        txt = re.sub(r';quot',\"\",txt)\n",
    "        txt = re.sub(r'\\&amp',\"\",txt)\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = txt.lower()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' . ', '.', txt).strip()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        return txt\n",
    "    return txt\n",
    "\n",
    "def preprocessa_abstract(corpus: str):\n",
    "    regIter = re.finditer(\"(<ABSTRACT>)(.+?)(</ABSTRACT>)\", corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = re.sub(',','', txt)\n",
    "        txt = re.sub(';',' ;', txt)\n",
    "        txt = re.sub('[ ]+',' ', txt)\n",
    "        txt = re.sub(r'\\([^)]*\\)', \"\",txt)\n",
    "        txt = re.sub(r'\\<(.*?)\\>',\"\",txt)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(' . ', '.', txt).strip()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        lista.append(txt)\n",
    "    return txt\n",
    "\n",
    "\n",
    "def preprocessa_conclusion(corpus: str):\n",
    "    regIter = re.finditer('(<SECTION.*Conclusion.*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = re.sub(',','', txt)\n",
    "        txt = re.sub(';','', txt)\n",
    "        txt = re.sub(':','', txt)\n",
    "        txt = re.sub('[ ]+',' ', txt)\n",
    "        txt = re.sub(r'\\([^)]*\\)', \"\",txt)\n",
    "        txt = re.sub(r'\\<(.*?)\\>',\"\",txt)\n",
    "        txt = re.sub(r'small\\&',\"\",txt)\n",
    "        txt = re.sub(r'tagger&amp',\"\",txt)\n",
    "        txt = re.sub(r';quot',\"\",txt)\n",
    "        txt = re.sub(r'\\&amp',\"\",txt)\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(' . ', '.', txt).strip()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        txt = txt.strip()\n",
    "        #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "        return txt\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T03:45:23.851860Z",
     "start_time": "2022-10-13T03:45:23.811719Z"
    }
   },
   "outputs": [],
   "source": [
    "abstract   = preprocessa_abstract(texto)\n",
    "intro      = preprocessa_intro(texto)\n",
    "conclusion = preprocessa_conclusion(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T03:45:26.266434Z",
     "start_time": "2022-10-13T03:45:26.230181Z"
    }
   },
   "outputs": [],
   "source": [
    "token_abstract   = preprocessa_abstract(texto).split()\n",
    "token_intro      = preprocessa_intro(texto).split()\n",
    "token_conclusion = preprocessa_conclusion(texto).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T03:45:28.827629Z",
     "start_time": "2022-10-13T03:45:28.801542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this paper describes.method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art.inter-sentence similarity is replaced by rank in the local context. boundary locations are discovered by divisive clustering.'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessa_abstract(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T03:45:31.783556Z",
     "start_time": "2022-10-13T03:45:31.758603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'paper',\n",
       " 'describes.method',\n",
       " 'for',\n",
       " 'linear',\n",
       " 'text',\n",
       " 'segmentation',\n",
       " 'which',\n",
       " 'is',\n",
       " 'twice',\n",
       " 'as',\n",
       " 'accurate',\n",
       " 'and',\n",
       " 'over',\n",
       " 'seven',\n",
       " 'times',\n",
       " 'as',\n",
       " 'fast',\n",
       " 'as',\n",
       " 'the',\n",
       " 'state-of-the-art.inter-sentence',\n",
       " 'similarity',\n",
       " 'is',\n",
       " 'replaced',\n",
       " 'by',\n",
       " 'rank',\n",
       " 'in',\n",
       " 'the',\n",
       " 'local',\n",
       " 'context.',\n",
       " 'boundary',\n",
       " 'locations',\n",
       " 'are',\n",
       " 'discovered',\n",
       " 'by',\n",
       " 'divisive',\n",
       " 'clustering.']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T03:45:34.630836Z",
     "start_time": "2022-10-13T03:45:34.612745Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = [token_abstract+token_intro+token_conclusion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T03:45:37.833765Z",
     "start_time": "2022-10-13T03:45:37.639933Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(tokens, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T03:45:41.152890Z",
     "start_time": "2022-10-13T03:45:41.055584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'of': 1,\n",
       " 'is': 2,\n",
       " 'and': 3,\n",
       " 'to': 4,\n",
       " 'in': 5,\n",
       " 'for': 6,\n",
       " 'our': 7,\n",
       " 'this': 8,\n",
       " 'similarity': 9,\n",
       " 'on': 10,\n",
       " 'segmentation': 11,\n",
       " 'rank': 12,\n",
       " 'are': 13,\n",
       " 'algorithm': 14,\n",
       " 'text': 15,\n",
       " 'by': 16,\n",
       " 'with': 17,\n",
       " 'that': 18,\n",
       " 'more': 19,\n",
       " 'has': 20,\n",
       " 'we': 21,\n",
       " 'topic': 22,\n",
       " 'ranking': 23,\n",
       " 'which': 24,\n",
       " 'word': 25,\n",
       " 'cosine': 26,\n",
       " 'than': 27,\n",
       " 'one': 28,\n",
       " 'measure': 29,\n",
       " 'boundaries.': 30,\n",
       " 'from': 31,\n",
       " 'as': 32,\n",
       " 'r98': 33,\n",
       " 'an': 34,\n",
       " 'two': 35,\n",
       " 'activation': 36,\n",
       " 'speed': 37,\n",
       " 'due': 38,\n",
       " 'be': 39,\n",
       " 'clustering': 40,\n",
       " 'accuracy': 41,\n",
       " 'spread': 42,\n",
       " 'described': 43,\n",
       " 'image': 44,\n",
       " 'number': 45,\n",
       " 'it': 46,\n",
       " 'use': 47,\n",
       " 'given': 48,\n",
       " 'method': 49,\n",
       " 'strategy': 50,\n",
       " 'accuracy.': 51,\n",
       " 'scheme': 52,\n",
       " 'values': 53,\n",
       " 'segments': 54,\n",
       " 'lexical': 55,\n",
       " 'algorithm.': 56,\n",
       " 'h94': 57,\n",
       " 'performance': 58,\n",
       " 'was': 59,\n",
       " 'algorithms': 60,\n",
       " 'used': 61,\n",
       " 'similar': 62,\n",
       " 'density': 63,\n",
       " 'my': 64,\n",
       " 'compare': 65,\n",
       " 'work': 66,\n",
       " 'divisive': 67,\n",
       " 'multi-source': 68,\n",
       " 'methods': 69,\n",
       " 'time': 70,\n",
       " 'each': 71,\n",
       " 'mask': 72,\n",
       " 'results.': 73,\n",
       " 'although': 74,\n",
       " 'boundaries': 75,\n",
       " 'were': 76,\n",
       " 'or': 77,\n",
       " 'different': 78,\n",
       " 'c99': 79,\n",
       " 'coefficient': 80,\n",
       " 'increase': 81,\n",
       " 'their': 82,\n",
       " 'value': 83,\n",
       " 'all': 84,\n",
       " 'help': 85,\n",
       " 'document': 86,\n",
       " 'paper': 87,\n",
       " 'versions': 88,\n",
       " 'three': 89,\n",
       " 'result': 90,\n",
       " 'k98': 91,\n",
       " 'would': 92,\n",
       " 'matrix.': 93,\n",
       " 'semantic': 94,\n",
       " 'version': 95,\n",
       " 'process': 96,\n",
       " 'been': 97,\n",
       " 'precise': 98,\n",
       " 'experimental': 99,\n",
       " 'accurate': 100,\n",
       " 'matrix': 101,\n",
       " 'implementation': 102,\n",
       " 'most': 103,\n",
       " 'corpus': 104,\n",
       " 'include': 105,\n",
       " 'can': 106,\n",
       " 'procedure': 107,\n",
       " 'uses': 108,\n",
       " 'not': 109,\n",
       " 'does': 110,\n",
       " 'b&#8222': 111,\n",
       " 'real': 112,\n",
       " 'demonstrate': 113,\n",
       " 'shows': 114,\n",
       " 'effect': 115,\n",
       " \"kozima's\": 116,\n",
       " 'improve': 117,\n",
       " 'let': 118,\n",
       " 'tdt': 119,\n",
       " 'four': 120,\n",
       " 'stem': 121,\n",
       " 'computed': 122,\n",
       " 'using': 123,\n",
       " 'independent': 124,\n",
       " 'evaluation': 125,\n",
       " 'results': 126,\n",
       " 'sentence': 127,\n",
       " 'experiments': 128,\n",
       " 'elements': 129,\n",
       " 'dot': 130,\n",
       " 'statistics': 131,\n",
       " 'order': 132,\n",
       " 'automatic': 133,\n",
       " 'boundary': 134,\n",
       " 'behaviour': 135,\n",
       " 'qualitative': 136,\n",
       " 'based': 137,\n",
       " 'maximisation': 138,\n",
       " 'data': 139,\n",
       " 'cohesion': 140,\n",
       " 'frequency': 141,\n",
       " 'improved': 142,\n",
       " 'sum': 143,\n",
       " 'inside': 144,\n",
       " 'sliding': 145,\n",
       " 'typically': 146,\n",
       " 'into': 147,\n",
       " 'window': 148,\n",
       " 'chains': 149,\n",
       " 'produce': 150,\n",
       " 'regular': 151,\n",
       " 'metric': 152,\n",
       " 'measures': 153,\n",
       " 'task': 154,\n",
       " 'error': 155,\n",
       " 'probability': 156,\n",
       " 'indicates': 157,\n",
       " 'insignificant': 158,\n",
       " 'area': 159,\n",
       " 'jeffrey': 160,\n",
       " 'department': 161,\n",
       " 'segment.': 162,\n",
       " 'five': 163,\n",
       " 'reynar': 164,\n",
       " 'any': 165,\n",
       " 'making': 166,\n",
       " 'potential': 167,\n",
       " 'implementations': 168,\n",
       " 'measured': 169,\n",
       " 'possible': 170,\n",
       " 'programming': 171,\n",
       " 'model': 172,\n",
       " 'context': 173,\n",
       " 'repetition': 174,\n",
       " 'largely': 175,\n",
       " 'applied': 176,\n",
       " 'focus': 177,\n",
       " 'step': 178,\n",
       " 'oram': 179,\n",
       " 'ak': 180,\n",
       " 'references': 181,\n",
       " 'coherent': 182,\n",
       " 'processing': 183,\n",
       " 'sij': 184,\n",
       " 'diagonal': 185,\n",
       " 'along': 186,\n",
       " 'segment': 187,\n",
       " \"reynar's\": 188,\n",
       " 'but': 189,\n",
       " 'techniques': 190,\n",
       " 'contrast': 191,\n",
       " 'magnus': 192,\n",
       " 'figure': 193,\n",
       " 'rattray': 194,\n",
       " 'stephen': 195,\n",
       " 'with.lower': 196,\n",
       " 'mask.': 197,\n",
       " 'used.11.11': 198,\n",
       " 'marsland': 199,\n",
       " 'range': 200,\n",
       " 'output': 201,\n",
       " 'section': 202,\n",
       " 'daniel': 203,\n",
       " 'table.presents': 204,\n",
       " 'den)': 205,\n",
       " 'stories': 206,\n",
       " 'discussions': 207,\n",
       " 'guidance': 208,\n",
       " 'problem': 209,\n",
       " 'sentences': 210,\n",
       " 'hideki': 211,\n",
       " 'words': 212,\n",
       " 'kozima': 213,\n",
       " 'tokens': 214,\n",
       " 'represented': 215,\n",
       " 'denote': 216,\n",
       " 'performance.': 217,\n",
       " 'min-yen': 218,\n",
       " 'absolute': 219,\n",
       " 'kan': 220,\n",
       " 'sim': 221,\n",
       " 'unreliable.': 222,\n",
       " 'suggests': 223,\n",
       " 'marti': 224,\n",
       " 'reduction': 225,\n",
       " 'unusually': 226,\n",
       " 'hearst': 227,\n",
       " 'mathematics.': 228,\n",
       " 'parents': 229,\n",
       " 'key': 230,\n",
       " 'thanks': 231,\n",
       " 'propose': 232,\n",
       " 'linearises': 233,\n",
       " 'performed': 234,\n",
       " 'information': 235,\n",
       " 'language': 236,\n",
       " 'focuses': 237,\n",
       " 'coefficient.': 238,\n",
       " 'proposed': 239,\n",
       " 'chain': 240,\n",
       " 'insufficient': 241,\n",
       " 'segmenting': 242,\n",
       " 'written': 243,\n",
       " 'indeed': 244,\n",
       " 'reliable': 245,\n",
       " 'actual': 246,\n",
       " 'significantly': 247,\n",
       " 'values.': 248,\n",
       " 'sufficient': 249,\n",
       " 'comparative': 250,\n",
       " 'study': 251,\n",
       " 'further': 252,\n",
       " 'research': 253,\n",
       " 'equation': 254,\n",
       " 'matrix.is': 255,\n",
       " 'requires.large': 256,\n",
       " 'scale': 257,\n",
       " 'improvement': 258,\n",
       " 'significant': 259,\n",
       " 'topic.': 260,\n",
       " 'approach': 261,\n",
       " 'elements.clustering': 262,\n",
       " 'linear': 263,\n",
       " 'and.similarity': 264,\n",
       " 'measure.': 265,\n",
       " 'show': 266,\n",
       " 'locating': 267,\n",
       " 'examined.': 268,\n",
       " 'yield': 269,\n",
       " 'measure.improved': 270,\n",
       " 'replaced': 271,\n",
       " 'confirms': 272,\n",
       " 'local': 273,\n",
       " 'same': 274,\n",
       " 'size': 275,\n",
       " 'computationally': 276,\n",
       " 'expensive': 277,\n",
       " 'clustering.': 278,\n",
       " 'long': 279,\n",
       " 'termination': 280,\n",
       " 'impact': 281,\n",
       " 'experiment': 282,\n",
       " 'both': 283,\n",
       " 'c99.': 284,\n",
       " 'segmentation.': 285,\n",
       " 'benchmark.': 286,\n",
       " 'breaking': 287,\n",
       " 'exact': 288,\n",
       " 'develop.linear': 289,\n",
       " 'thus': 290,\n",
       " 'corpus.': 291,\n",
       " 'only': 292,\n",
       " 'optimisations': 293,\n",
       " 'similarity.': 294,\n",
       " 'existing': 295,\n",
       " 'also': 296,\n",
       " 'like': 297,\n",
       " 'of.different': 298,\n",
       " 'short': 299,\n",
       " 'benefitted': 300,\n",
       " 'comments': 301,\n",
       " 'mary': 302,\n",
       " 'mcgee': 303,\n",
       " 'wood': 304,\n",
       " 'former': 305,\n",
       " 'anonymous': 306,\n",
       " 'reviewers.': 307,\n",
       " 'interesting': 308,\n",
       " 'other': 309,\n",
       " 'spoken': 310,\n",
       " 'broadcast': 311,\n",
       " 'cohesive': 312,\n",
       " 'than.section': 313,\n",
       " 'about.particular': 314,\n",
       " 'several': 315,\n",
       " 'consequently': 316,\n",
       " 'address': 317,\n",
       " 'of.ranking': 318,\n",
       " 'inappropriate': 319,\n",
       " 'directly': 320,\n",
       " 'documents': 321,\n",
       " 'regions': 322,\n",
       " 'models.work': 323,\n",
       " 'non-parametric': 324,\n",
       " 'statistical': 325,\n",
       " 'distance': 326,\n",
       " 'phrases': 327,\n",
       " 'entity': 328,\n",
       " 'moderately': 329,\n",
       " 'analysis': 330,\n",
       " 'topics': 331,\n",
       " 'less': 332,\n",
       " 'of.document': 333,\n",
       " 'cohesion.': 334,\n",
       " '&lt': 335,\n",
       " '100': 336,\n",
       " 'categories': 337,\n",
       " 'informative': 338,\n",
       " 'finding': 339,\n",
       " 'estimate': 340,\n",
       " 'between': 341,\n",
       " 'e.g..is': 342,\n",
       " 'to.than': 343,\n",
       " 'detect': 344,\n",
       " 'c.': 345,\n",
       " 'furthermore': 346,\n",
       " 'usage': 347,\n",
       " 'varies': 348,\n",
       " 'throughout.document.': 349,\n",
       " 'aspects': 350,\n",
       " 'instance': 351,\n",
       " 'introduction': 352,\n",
       " 'compares': 353,\n",
       " 'even': 354,\n",
       " 'sets': 355,\n",
       " 'twice': 356,\n",
       " 'fast': 357,\n",
       " 'region.': 358,\n",
       " 'times': 359,\n",
       " 'seven': 360,\n",
       " 'over': 361,\n",
       " 'reference': 362,\n",
       " 'of.coherent': 363,\n",
       " 'neighbouring': 364,\n",
       " 'part': 365,\n",
       " 'its': 366,\n",
       " 'value.': 367,\n",
       " 'figure.shows': 368,\n",
       " 'halliday': 369,\n",
       " 'hasan.they': 370,\n",
       " 'example': 371,\n",
       " 'likely': 372,\n",
       " 'vocabulary': 373,\n",
       " 'describes.method': 374,\n",
       " 'decision': 375,\n",
       " 'state-of-the-art.inter-sentence': 376,\n",
       " 'vectors': 377,\n",
       " 'present.ranking': 378,\n",
       " 'when': 379,\n",
       " 'probabilistic': 380,\n",
       " 'discovered': 381,\n",
       " 'methods.the': 382,\n",
       " 'locations': 383,\n",
       " 'prosodic': 384,\n",
       " 'context.': 385,\n",
       " 'quantities': 386,\n",
       " 'adaptation': 387,\n",
       " 'formulating': 388,\n",
       " 'idea': 389,\n",
       " \"in.'the\": 390,\n",
       " 'features': 391,\n",
       " 'using.3.3': 392,\n",
       " 'trees': 393,\n",
       " 'adjusted': 394,\n",
       " 'highlight': 395,\n",
       " 'features.': 396,\n",
       " 'aim': 397,\n",
       " 'cue': 398,\n",
       " 'where.segment': 399,\n",
       " 'uninformative': 400,\n",
       " 'acceptable': 401,\n",
       " 'input': 402,\n",
       " 'statistically': 403,\n",
       " 'format.': 404,\n",
       " 'in.collection': 405,\n",
       " 'insignificant.': 406,\n",
       " 'punctuation': 407,\n",
       " 'present.new': 408,\n",
       " 'transcribed': 409,\n",
       " 'builds': 410,\n",
       " 'text.': 411,\n",
       " 'removed': 412,\n",
       " 'shift': 413,\n",
       " 'initiative.the': 414,\n",
       " 'using.simple': 415,\n",
       " 'expression': 416,\n",
       " 'pattern': 417,\n",
       " 'matcher': 418,\n",
       " 'convert.plain': 419,\n",
       " 'may': 420,\n",
       " 'such': 421,\n",
       " 'distinction': 422,\n",
       " 'news': 423,\n",
       " 'combine': 424,\n",
       " 'where': 425,\n",
       " 'presentation': 426,\n",
       " 'format': 427,\n",
       " 'indicators': 428,\n",
       " 'cues': 429,\n",
       " 'exploited': 430,\n",
       " 'takes.list': 431,\n",
       " 'eagle': 432,\n",
       " 'tokenized': 433,\n",
       " 'primary': 434,\n",
       " 'reynar.the': 435,\n",
       " 'retrieval.multi-source': 436,\n",
       " 'input..tokenizer': 437,\n",
       " 'and.sentence': 438,\n",
       " 'previous': 439,\n",
       " 'disambiguation': 440,\n",
       " 'and.stopword': 441,\n",
       " 'list..stemming': 442,\n",
       " 'large.': 443,\n",
       " 'dynamic': 444,\n",
       " 'between.pair': 445,\n",
       " 'sentences.y': 446,\n",
       " 'resolution': 447,\n",
       " 'anaphora': 448,\n",
       " 'understanding': 449,\n",
       " 'summarization': 450,\n",
       " 'retrieval': 451,\n",
       " 'falls': 452,\n",
       " 'attraction': 453,\n",
       " 'i.': 454,\n",
       " 'additional': 455,\n",
       " 'motivated': 456,\n",
       " 'occurrence': 457,\n",
       " 'of.common': 458,\n",
       " 'causes.disproportionate': 459,\n",
       " 'discover': 460,\n",
       " 'unless': 461,\n",
       " 'denominator': 462,\n",
       " 'agglomerative': 463,\n",
       " 'word.in': 464,\n",
       " 'domain': 465,\n",
       " 'constructed': 466,\n",
       " 'rely': 467,\n",
       " 'then': 468,\n",
       " 'tracking': 469,\n",
       " 'remaining': 470,\n",
       " 'clustering.lexical': 471,\n",
       " 'obtain': 472,\n",
       " 'stems..dictionary': 473,\n",
       " 'frequencies': 474,\n",
       " 'sentence.': 475,\n",
       " 'modelling': 476,\n",
       " 'detection': 477,\n",
       " 'as.vector': 478,\n",
       " 'counts.': 479,\n",
       " 'disabled.this': 480,\n",
       " 'fii': 481,\n",
       " 'visually': 482,\n",
       " 'navigation': 483,\n",
       " 'improving': 484,\n",
       " 'syntax': 485,\n",
       " '.segmentation': 486,\n",
       " '{0': 487,\n",
       " 'measuring': 488,\n",
       " 'compute': 489,\n",
       " 'uses.variant': 490,\n",
       " 'measures.': 491,\n",
       " 'experimenting': 492,\n",
       " 'modularised': 493,\n",
       " '3.4.': 494,\n",
       " 'incorporates': 495,\n",
       " 'instead': 496,\n",
       " 'h94.': 497,\n",
       " 'minimisation': 498,\n",
       " 'his': 499,\n",
       " 'evaluated.': 500,\n",
       " 'optimisation': 501,\n",
       " 'stemming': 502,\n",
       " 'list': 503,\n",
       " 'stopword': 504,\n",
       " 'block': 505,\n",
       " 'is.function': 506,\n",
       " 'cooccurrence': 507,\n",
       " 'document.': 508,\n",
       " 'experiment.': 509,\n",
       " 'matrix..=.was': 510,\n",
       " 'converts.matrix.into.transition': 511,\n",
       " 'norm': 512,\n",
       " 'steps': 513,\n",
       " 'scheme..denotes': 514,\n",
       " 'defines': 515,\n",
       " '11': 516,\n",
       " '10.': 517,\n",
       " 'transition': 518,\n",
       " 'frequencies.the': 519,\n",
       " 'co-occurrence': 520,\n",
       " 'related.': 521,\n",
       " 'considered': 522,\n",
       " 'belong': 523,\n",
       " 'suspect': 524,\n",
       " 'parameters.=.w.20.': 525,\n",
       " 'execution': 526,\n",
       " 'consider': 527,\n",
       " 'equation.8': 528,\n",
       " 'to.and.=.-p.': 529,\n",
       " 'equation.corresponds': 530,\n",
       " 'terms.awl.in': 531,\n",
       " 'string.the': 532,\n",
       " 'as.bit': 533,\n",
       " 'status': 534,\n",
       " 'analytically.': 535,\n",
       " 'recommended': 536,\n",
       " 'last': 537,\n",
       " 'selects.boundaries': 538,\n",
       " 'boundaries..randomly': 539,\n",
       " 'selects': 540,\n",
       " 'segments..randomly': 541,\n",
       " 'sample': 542,\n",
       " 'partitions': 543,\n",
       " 'and.gives': 544,\n",
       " 'general': 545,\n",
       " 'form': 546,\n",
       " 'of.b': 547,\n",
       " 'parameters.': 548,\n",
       " 'default': 549,\n",
       " \"hearst's.implementation\": 550,\n",
       " 'algorithm.h94': 551,\n",
       " 'texttiling': 552,\n",
       " 'ks-test.we': 553,\n",
       " 'kolmogorov-smirnov': 554,\n",
       " 'according': 555,\n",
       " 'differences': 556,\n",
       " 'however': 557,\n",
       " 'same.': 558,\n",
       " 'actually': 559,\n",
       " 'row': 560,\n",
       " \"respectively'.\": 561,\n",
       " 'berm': 562,\n",
       " 'reduced': 563,\n",
       " 'time.': 564,\n",
       " 'experiments.': 565,\n",
       " 'investigates': 566,\n",
       " 'beyond.x.has': 567,\n",
       " 'interestingly': 568,\n",
       " 'zero.': 569,\n",
       " 'reduces': 570,\n",
       " 'size..1.1': 571,\n",
       " 'increases': 572,\n",
       " 'c99.execution': 573,\n",
       " 'second': 574,\n",
       " 'comparison': 575,\n",
       " 'acceptable.': 576,\n",
       " 'minor': 577,\n",
       " 'optimal.': 578,\n",
       " 'effective': 579,\n",
       " 'marginally': 580,\n",
       " 'c99.c99': 581,\n",
       " 'first': 582,\n",
       " 'extrema': 583,\n",
       " 'has.greater': 584,\n",
       " 'linearising': 585,\n",
       " 'scores.experimental': 586,\n",
       " 'ks-test..segmentation': 587,\n",
       " 't-test': 588,\n",
       " 'confirmed': 589,\n",
       " 'significance': 590,\n",
       " 'algorithms.': 591,\n",
       " 'polynomial': 592,\n",
       " 'performance.c99': 593,\n",
       " 'algorithmic': 594,\n",
       " 'best': 595,\n",
       " 'disregards': 596,\n",
       " 'if': 597,\n",
       " 'r98).': 598,\n",
       " 'achieved': 599,\n",
       " 'seven-fold': 600,\n",
       " 'algorithms..two-fold': 601,\n",
       " 'r98.': 602,\n",
       " 'fair': 603,\n",
       " 'segmenter.k98': 604,\n",
       " 'link': 605,\n",
       " 'mean.and': 606,\n",
       " 'vt': 607,\n",
       " 'is.function.+.x': 608,\n",
       " 'threshold': 609,\n",
       " 'links.': 610,\n",
       " 'identify': 611,\n",
       " 'distances': 612,\n",
       " 'distribution': 613,\n",
       " 'expected': 614,\n",
       " 'strategy.': 615,\n",
       " 'specific': 616,\n",
       " 'uses.document': 617,\n",
       " 'is.version': 618,\n",
       " 'algorithm.k98': 619,\n",
       " 'perl': 620,\n",
       " 'original': 621,\n",
       " 'variance': 622,\n",
       " 'found.=.works': 623,\n",
       " 'well': 624,\n",
       " 'practice.': 625,\n",
       " 'latter': 626,\n",
       " 'paper.': 627,\n",
       " 'developed': 628,\n",
       " 'k98)&#8226': 629,\n",
       " 'strategies.': 630,\n",
       " 'term': 631,\n",
       " 'languages': 632,\n",
       " 'difference': 633,\n",
       " 'parser.': 634,\n",
       " 'shallow': 635,\n",
       " 'tagger': 636,\n",
       " 'part-of-speech': 637,\n",
       " 'k98.': 638,\n",
       " 'better': 639,\n",
       " 'table.summarises': 640,\n",
       " 'reports': 641,\n",
       " 'baseline': 642,\n",
       " '8).': 643,\n",
       " 'point': 644,\n",
       " 'determined': 645,\n",
       " 'generate': 646,\n",
       " 'example.': 647,\n",
       " 'figure.shows.working': 648,\n",
       " 'd.': 649,\n",
       " 'maximises': 650,\n",
       " 'is.potential': 651,\n",
       " 'split': 652,\n",
       " 'segment.in': 653,\n",
       " 'b.': 654,\n",
       " 'splits': 655,\n",
       " 'in.as': 656,\n",
       " 'placed': 657,\n",
       " 'entire': 658,\n",
       " 'initialise': 659,\n",
       " 'of..': 660,\n",
       " 'automatically.': 661,\n",
       " 'of.segments': 662,\n",
       " 'sd': 663,\n",
       " 'gradient.': 664,\n",
       " 'computation': 665,\n",
       " 'dominated': 666,\n",
       " 'running': 667,\n",
       " 'obtained3': 668,\n",
       " 'optiinal': 669,\n",
       " '6d': 670,\n",
       " 'large': 671,\n",
       " 'od}.an': 672,\n",
       " '{bd': 673,\n",
       " '...d}': 674,\n",
       " '{d': 675,\n",
       " 'generates': 676,\n",
       " 'boundaries.steps': 677,\n",
       " 'with.potential': 678,\n",
       " 'for.document': 679,\n",
       " 'b..is': 680,\n",
       " 'refers': 681,\n",
       " 'constant': 682,\n",
       " 'notice': 683,\n",
       " 'wave': 684,\n",
       " 'is.sine': 685,\n",
       " 'scheme..is': 686,\n",
       " 'effects': 687,\n",
       " 'subtle': 688,\n",
       " 'figure.illustrates': 689,\n",
       " 'significantly.': 690,\n",
       " '32.': 691,\n",
       " 'sk': 692,\n",
       " 'figure.to': 693,\n",
       " 'shown': 694,\n",
       " 'problems.#': 695,\n",
       " 'normalisation': 696,\n",
       " 'circumvent': 697,\n",
       " 'as.ratio.to': 698,\n",
       " 'expressed': 699,\n",
       " 'decaying': 700,\n",
       " 'mean': 701,\n",
       " 'amplitude': 702,\n",
       " 'frequency.the': 703,\n",
       " 'segments.': 704,\n",
       " 'is.list': 705,\n",
       " '...197-4': 706,\n",
       " '{b1': 707,\n",
       " 'area..=': 708,\n",
       " 'aij.2': 709,\n",
       " 'in.segment': 710,\n",
       " 'region': 711,\n",
       " 'as.square': 712,\n",
       " 'sentences.j.this': 713,\n",
       " 'defined': 714,\n",
       " 'algorithm.a': 715,\n",
       " 'location': 716,\n",
       " 'determines': 717,\n",
       " 'final': 718,\n",
       " 'sk.': 719,\n",
       " 'pre-computes': 720,\n",
       " 'define': 721,\n",
       " 'generated': 722,\n",
       " 'seconds': 723,\n",
       " 'cpu': 724,\n",
       " 'average': 725,\n",
       " 'pp': 726,\n",
       " 'pp+': 727,\n",
       " 'statistics..=': 728,\n",
       " 'procedure5.': 729,\n",
       " 'n.': 730,\n",
       " 'is.concatenation': 731,\n",
       " 'characterised': 732,\n",
       " \"corpus'..sample\": 733,\n",
       " 'brown': 734,\n",
       " 'selected': 735,\n",
       " 'of.randomly': 736,\n",
       " 'first.sentences': 737,\n",
       " 'segments..segment': 738,\n",
       " 'required': 739,\n",
       " 'process.test': 740,\n",
       " 'sample6.': 741,\n",
       " 'in.low': 742,\n",
       " 'degenerate': 743,\n",
       " 'in.five': 744,\n",
       " 'discussed': 745,\n",
       " 'metrics': 746,\n",
       " 'these': 747,\n",
       " 'associated': 748,\n",
       " 'problems': 749,\n",
       " 'distance.the': 750,\n",
       " 'edit': 751,\n",
       " 'pr': 752,\n",
       " 'fuzzy': 753,\n",
       " 'recall': 754,\n",
       " 'precision': 755,\n",
       " 'popular': 756,\n",
       " 'high': 757,\n",
       " 'ten': 758,\n",
       " 'algorithms..sample': 759,\n",
       " 'computes': 760,\n",
       " 'rij': 761,\n",
       " 'applying': 762,\n",
       " 'steps.figure.shows': 763,\n",
       " '77.is': 764,\n",
       " 'size.x': 765,\n",
       " 'given.of': 766,\n",
       " 'matrix.and.to': 767,\n",
       " 'refer': 768,\n",
       " '171-5.n2.': 769,\n",
       " 'assess': 770,\n",
       " 'has.complexity': 771,\n",
       " 'corner.': 772,\n",
       " 'towards': 773,\n",
       " 'works': 774,\n",
       " 'main': 775,\n",
       " 'starting': 776,\n",
       " 'diagonals': 777,\n",
       " '5.': 778,\n",
       " 'definition': 779,\n",
       " 'of.topic': 780,\n",
       " 'ranges': 781,\n",
       " 'samples': 782,\n",
       " '700': 783,\n",
       " 'test': 784,\n",
       " 'artificial': 785,\n",
       " 'prominent': 786,\n",
       " 'finds': 787,\n",
       " 'assume.good': 788,\n",
       " 'task.we': 789,\n",
       " 'is.variant': 790,\n",
       " 'relative': 791,\n",
       " 'following': 792,\n",
       " 'dependent': 793,\n",
       " 'quality': 794,\n",
       " 'summaries.given': 795,\n",
       " 'complete': 796,\n",
       " 'den-1)': 797}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T03:20:33.469314Z",
     "start_time": "2022-10-13T03:20:33.453499Z"
    }
   },
   "source": [
    "frases_abstract = abstract.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T03:45:44.013967Z",
     "start_time": "2022-10-13T03:45:43.990132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this paper describes',\n",
       " ' method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art',\n",
       " ' inter-sentence similarity is replaced by rank in the local context',\n",
       " ' boundary locations are discovered by divisive clustering',\n",
       " '']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frases_abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frases_abstract = abstract.split('.')\n",
    "palavras_frases=[]\n",
    "for i in frases_abstract:\n",
    "    palavras_frases.append(i.split(' '))\n",
    "palavras_frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T03:53:34.233552Z",
     "start_time": "2022-10-13T03:53:34.214601Z"
    }
   },
   "outputs": [],
   "source": [
    "def funcao_frase_palavra(abstract):\n",
    "    \"\"\"input: lista de frases\n",
    "        output: lista de palavras de cada frase\"\"\"\n",
    "    frases_abstract = abstract.split('.')\n",
    "    palavras_frases=[]\n",
    "    for i in frases_abstract:\n",
    "        palavras_frases.append(i.split(' '))\n",
    "    try:\n",
    "        palavras_frases.remove('')\n",
    "    except:\n",
    "        return palavras_frases\n",
    "    return palavras_frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T03:54:16.443549Z",
     "start_time": "2022-10-13T03:54:16.424567Z"
    }
   },
   "outputs": [],
   "source": [
    "def vetor_frases(palavras_frases):\n",
    "    \"\"\"input: lista de palavras de cada frase\n",
    "        output: lista de vetores de cada frase\"\"\"\n",
    "    x=[]\n",
    "    for i in palavras_frases:\n",
    "        soma=0\n",
    "        for j in i:\n",
    "            try:\n",
    "                soma = soma +model.wv[j]\n",
    "            except:\n",
    "                soma=soma+0\n",
    "        x.append(soma)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T04:00:09.998030Z",
     "start_time": "2022-10-13T04:00:09.980074Z"
    }
   },
   "outputs": [],
   "source": [
    "frases_abstract = abstract.split('.')\n",
    "palavras_abstract = funcao_frase_palavra(abstract)\n",
    "try:\n",
    "    palavras_abstract.remove([''])\n",
    "except:\n",
    "    None\n",
    "vetor_abstract = vetor_frases(palavras_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T04:00:25.280981Z",
     "start_time": "2022-10-13T04:00:25.249067Z"
    }
   },
   "outputs": [],
   "source": [
    "frases_intro = intro.split('.').remove('')\n",
    "palavras_intro = funcao_frase_palavra(intro)\n",
    "try:\n",
    "    palavras_intro.remove([''])\n",
    "except:\n",
    "    None\n",
    "vetor_intro = vetor_frases(palavras_intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T04:02:40.576045Z",
     "start_time": "2022-10-13T04:02:40.555132Z"
    }
   },
   "outputs": [],
   "source": [
    "frases_conclusion = conclusion.split('.').remove('')\n",
    "palavras_conclusion = funcao_frase_palavra(conclusion)\n",
    "vetor_conclusion = vetor_frases(palavras_conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T04:03:03.707021Z",
     "start_time": "2022-10-13T04:03:03.692063Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-310-c480b0658c8a>:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cos_sim = dot(a, b)/(norm(a)*norm(b))\n"
     ]
    }
   ],
   "source": [
    "h=[]\n",
    "for i in range(len(vetor_conclusion)):\n",
    "    x=[]\n",
    "    for j in range(len(vetor_abstract)):\n",
    "        x.append(funcao_sim_coseno(vetor_conclusion[i],vetor_abstract[j]))\n",
    "    h.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T04:03:29.151398Z",
     "start_time": "2022-10-13T04:03:29.137436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.11343461, 0.33072603, 0.24621186, -0.013001951],\n",
       " [0.13558297, 0.35002366, 0.294813, 0.30642053],\n",
       " [0.114904344, 0.2264548, 0.31665838, 0.22883265],\n",
       " [0.3164524, 0.48683062, 0.51535493, 0.34803754],\n",
       " [0.06724528, 0.08179665, 0.21793294, 0.12674674],\n",
       " [0.23069417, 0.50609446, 0.5427517, 0.2817563],\n",
       " [0.19042166, 0.38135242, 0.27701235, 0.24419267],\n",
       " [0.07874766, 0.20818742, 0.18146408, 0.19055465],\n",
       " [0.36211255, 0.5254655, 0.5273954, 0.18882833],\n",
       " [0.22723486, 0.62126356, 0.62536186, 0.30707997],\n",
       " [0.3635971, 0.6533926, 0.7059151, 0.2644407],\n",
       " [0.46812963, 0.558244, 0.57537305, 0.07276361],\n",
       " [0.07926293, 0.08658707, 0.104880735, 0.17139836],\n",
       " [0.30810407, 0.5897009, 0.67089987, 0.34568548],\n",
       " [0.14256617, 0.37697527, 0.37271166, 0.30766088],\n",
       " [0.2683056, 0.57055265, 0.4732306, 0.23750886],\n",
       " [0.48715988, 0.5753263, 0.5687768, 0.22283491],\n",
       " [0.33668265, 0.66536903, 0.57132685, 0.2870629],\n",
       " [array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan]),\n",
       "  array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan]),\n",
       "  array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan]),\n",
       "  array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan])]]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-13T02:32:16.997Z"
    }
   },
   "outputs": [],
   "source": [
    "cosine_similarity = numpy.dot(soma, soma2)/(numpy.linalg.norm(soma)* numpy.linalg.norm(soma2))\n",
    "cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-13T02:32:16.999Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(word_vec):\n",
    "    norm=np.linalg.norm(word_vec)\n",
    "    if norm == 0: \n",
    "        return word_vec\n",
    "    return word_vec/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-13T02:32:17.002Z"
    }
   },
   "outputs": [],
   "source": [
    "a=soma\n",
    "b=soma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-13T02:32:17.004Z"
    }
   },
   "outputs": [],
   "source": [
    "a=normalize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-13T02:32:17.006Z"
    }
   },
   "outputs": [],
   "source": [
    "b=normalize(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-13T02:32:17.008Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
